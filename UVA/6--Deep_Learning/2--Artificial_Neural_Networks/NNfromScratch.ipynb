{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Neural Network from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "#!pip install mnist\n",
    "import mnist\n",
    "%matplotlib inline\n",
    "plt.style.use('default')\n",
    "\n",
    "# load data\n",
    "train_images = mnist.train_images()\n",
    "train_labels = mnist.train_labels()\n",
    "num_train_images = len(train_labels)\n",
    "test_images = mnist.test_images()\n",
    "test_labels = mnist.test_labels()\n",
    "num_test_images = len(test_images)\n",
    "\n",
    "# print the data dimensions\n",
    "print(\"Train Images Shape: \"+str(train_images.shape))\n",
    "print(\"Train Labels Shape: \"+str(train_labels.shape))\n",
    "print('Train Images DataType: '+str(train_images.dtype))\n",
    "print(\"Test Images Shape: \"+str(test_images.shape))\n",
    "print(\"Test Labels Shape: \"+str(test_labels.shape))\n",
    "print('Test Images DataType: '+str(test_images.dtype))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display a single sample\n",
    "idx = 2\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.imshow(train_images[idx,:,:], cmap='gray')\n",
    "plt.title(\"Image Label: \"+str(train_labels[idx]));"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a diagram of the neural network we will build:\n",
    "\n",
    "<img src=\"Network_Diagram.png\" width=\"800\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the components of the network:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ReLU function in\n",
    "\\begin{equation}\n",
    "ReLU(x) = \n",
    "\\begin{cases} \n",
    "      0 & \\textrm{if} & x < 0 \\\\\n",
    "      x & \\textrm{if} & x > 0 \n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "The derivative is \n",
    "\\begin{equation}\n",
    "\\frac{\\textrm{d}ReLU(x)}{\\textrm{dx}} = \n",
    "\\begin{cases} \n",
    "      0 & \\textrm{if} & x < 0 \\\\\n",
    "      1 & \\textrm{if} & x > 0 \n",
    "\\end{cases}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the nonlinear ReLU function\n",
    "def ReLU(x):\n",
    "  return np.max((0,x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The softmax function is\n",
    "\\begin{equation}\n",
    "\\sigma(x)=(\\sigma_1(x),\\sigma_2(x),...,\\sigma_m(x)), \\textrm{ where } \\sigma_i(x)=\\frac{e^{x_i}}{\\sum_j e^{x_j}}.\n",
    "\\end{equation}\n",
    "Note that the components sum to 1, so this is a probability distribution."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that for $i\\neq j$:\n",
    "\\begin{equation}\n",
    "\\frac{d \\sigma_j(x)}{d x_i} = -\\sigma_j(x)\\sigma_i(x)\n",
    "\\end{equation}\n",
    "and \n",
    "\\begin{equation}\n",
    "\\frac{d \\sigma_j(x)}{d x_j} = \\sigma_j(x) - \\sigma_j(x)^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the softmax function.  \n",
    "# This converts a vector of K real numbers into a probability distribution of K possible outcomes. \n",
    "# It is a generalization of the logistic function to multiple dimensions, and used in multinomial logistic regression.\n",
    "def softmax(inputs):\n",
    "  outputs = np.exp(inputs)/np.sum(np.exp(inputs))\n",
    "  return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax(np.array([-1, 3, 12, -4]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can put these pieces together to make a layer in the form of a Python class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a layer class\n",
    "class FClayer:\n",
    "  \n",
    "  def __init__(self, numInputs, numNodes):\n",
    "    self.numInputs = numInputs\n",
    "    self.numNodes = numNodes\n",
    "    self.weights = np.random.normal(0.0, 0.05, size=(numNodes, numInputs))\n",
    "    self.biases = np.zeros(numNodes)\n",
    "\n",
    "  def apply(self, inputs):\n",
    "    if (len(inputs) != self.numInputs):\n",
    "      # check here so if there is an error we get told nicely about it.\n",
    "      print(\"WARNING: Inputs to layer wrong size for the layer.\")\n",
    "      output = np.zeros(self.numNodes)\n",
    "      return output\n",
    "    # create a vector to hour the output of the layer\n",
    "    output = np.zeros(self.numNodes)\n",
    "    # Apply weights and baises\n",
    "    x = np.matmul(self.weights, inputs) + self.biases\n",
    "    # Apply the nonlinear function to each node\n",
    "    for i in range(self.numNodes):\n",
    "      output[i] = ReLU(x[i])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = FClayer(5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "28*28"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define two hidden layers and an output layer\n",
    "layer1 = FClayer(28*28,30)\n",
    "layer2 = FClayer(30,10)\n",
    "layerFinal = softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the network\n",
    "class network2layers():\n",
    "  \n",
    "  def __init__(self, layer1, layer2, layerFinal):\n",
    "    self.layer1 = layer1\n",
    "    self.layer2 = layer2\n",
    "    self.layerFinal = layerFinal\n",
    "  \n",
    "  def apply(self, inputs):\n",
    "    self.inputs = inputs\n",
    "    # This determines the network 'architecture'\n",
    "    self.out1 = self.layer1.apply(self.inputs)\n",
    "    self.out2 = self.layer2.apply(self.out1)\n",
    "    self.outFinal = self.layerFinal(self.out2)\n",
    "    return self.outFinal\n",
    "\n",
    "  def train(self, obs, y, alpha):\n",
    "    # this is a placeholder method that we will modify later.\n",
    "    # for now we just set the parameters equal to themselves instead of updating them\n",
    "    #   obs = one observation (input into the network)\n",
    "    #   y = the truth value for the obs\n",
    "    #   alpha = the training rate\n",
    "    self.layer1.weights = self.layer1.weights\n",
    "    self.layer1.biases = self.layer1.biases\n",
    "    self.layer2.weights = self.layer2.weights\n",
    "    self.layer2.biases = self.layer2.biases\n",
    "\n",
    "# create an instance of this network \n",
    "# (the weights will be randomly selected since it is not trained)\n",
    "network = network2layers(layer1, layer2, layerFinal)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = [[0,255,0],[0,100,0],[0,0,0]]\n",
    "plt.imshow(im, cmap='gray')\n",
    "print(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the output of each layer for a test image\n",
    "idx = 0 # (use the fisrt image/observation)\n",
    "out1 = network.layer1.apply(train_images[idx,:,:].flatten()/255.)\n",
    "print(\"Layer 1 output: \"+str(out1))\n",
    "out2 = network.layer2.apply(out1)\n",
    "print(\"Layer 2 output: \"+str(out2))\n",
    "outFinal = network.layerFinal(out2)\n",
    "print(\"Final layer output: \"+str(outFinal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the output of the network applied to a single image\n",
    "# (this is the same as the output from the final layer above)\n",
    "network.apply(train_images[2,:,:].flatten()/255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can get individual layer outputs from the network (since we saved them within the network class in the apply method)\n",
    "print(\"Layer 1 output: \"+str(network.out1))\n",
    "print(\"Layer 2 output: \"+str(network.out2))\n",
    "print(\"Final layer output: \"+str(network.outFinal))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights are hard to interpret by just looking at the numbers, so we can show them as images to get some intuition.  Since this is a network for learning images, the weights formats as an image for each node can be insightful. (NOTE:  This is just noise since these are random untrained weights, but they should be more meaningful after training.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math.isqrt(10)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NNview(layer, **kwargs):\n",
    "    titleTxt = kwargs['title']\n",
    "    numIn = layer.numInputs\n",
    "    numNodes = layer.numNodes\n",
    "    numRows = int(np.ceil(numNodes/5)) # number of rows of images\n",
    "    if (numIn == math.isqrt(numIn)**2):\n",
    "      plt.figure(figsize=(8,int(2*numRows)))\n",
    "      plt.suptitle(titleTxt, fontsize=20)     \n",
    "      for i in range(numNodes):\n",
    "        plt.subplot(numRows,5,i+1) \n",
    "        imSide = int(np.sqrt(numIn))\n",
    "        plt.imshow(np.reshape(layer.weights[i,:],(imSide,imSide)))\n",
    "        plt.title('Weights for node '+str(i), fontsize=10)\n",
    "        plt.colorbar(orientation='horizontal')\n",
    "        plt.tight_layout()\n",
    "    else:\n",
    "      plt.figure(figsize=(6,2.5))\n",
    "      plt.suptitle(titleTxt, fontsize=20)    \n",
    "      plt.imshow(layer.weights)\n",
    "      plt.xlabel('weights')\n",
    "      plt.ylabel('node')\n",
    "      plt.colorbar(orientation='vertical')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "\n",
    "NNview(network.layer1, title='Node weights for first layer')\n",
    "NNview(network.layer2, title='Node weights for second layer')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To measure accuracy we will compare the output of the network, $\\hat{y}$, on an image to the true label for the image, $y$.  We will use an accuracy measure called cross entropy.\n",
    "\n",
    "The **cross entropy** loss function is\n",
    "\\begin{equation}\n",
    "L(y,\\hat{y}) = -\\sum_i^m y_i \\log(\\hat{y}_i) = -(y_1 \\log(\\hat{y}_1) + y_2 \\log(\\hat{y}_2) + \\cdots y_m \\log(\\hat{y}_m) )\n",
    "\\end{equation}\n",
    "where $y=(y_1,y_2,...,y_m)$ is the true value and $\\hat{y} = (\\hat{y}_1,\\hat{y}_2,...,\\hat{y}_m)$ is the predicted value.\n",
    "\n",
    "For example, if the image is of a number 5, $y=(0,0,0,0,0,1,0,0,0,0)$ and $L(y,\\hat{y})=-\\log(\\hat{y}_5)$.  In general, for an image of a number $k$, $L(y,\\hat{y})=-\\log(\\hat{y}_k)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our cross entropy loss function\n",
    "def Loss(y,yHat):\n",
    "  L = 0\n",
    "  m = len(y)\n",
    "  for i in range(m):\n",
    "    # modify yHat[i] so we don't get log of zero (undefined).\n",
    "    yHat[i] = max(10**(-10),yHat[i])\n",
    "    L = L + y[i]*np.log(yHat[i])\n",
    "  return -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute and print cross entropy loss for a single sample\n",
    "idx = 0\n",
    "y = np.zeros(10)\n",
    "y[train_labels[idx]] = 1\n",
    "yHat = network.apply(train_images[idx,:,:].flatten()/255.)\n",
    "print('Cross Entropy Loss: '+str(Loss(y,yHat)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1:  Compute formulas for the derivatives of the Loss function with respect to the parameters in the second layer, $W^2_{i,j}$ and $b^2_i$ for $i,j \\in \\{0,...,9\\}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative of the cross entropy loss function is:\n",
    "\\begin{equation}\n",
    "\\frac{d L(y,\\hat{y})}{d\\hat{y}_i} = -\\frac{y_i}{\\hat{y}_i}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derivative of Loss function with respect to the 10 inputs\n",
    "def dL_dyi(y,yHat):\n",
    "  m = len(y)\n",
    "  output = np.zeros(m)\n",
    "  for i in range(m):\n",
    "    output[i] = -y[i]/yHat[i]\n",
    "  return output\n",
    "\n",
    "print(dL_dyi(y,yHat))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derivative of the cross entropy loss function with respect to an output of the second hidden layer is:\n",
    "\\begin{equation}\n",
    "\\frac{d L(y,\\hat{y})}{d x^2_i}   =  \\sum_{j=1}^m \\frac{d L(y,\\hat{y})}{d \\hat{y}_j} \\frac{d \\hat{y}_j}{d x^2_i}    =  -\\sum_{j=1}^m \\frac{y_j}{\\hat{y}_j} \\frac{d \\sigma_j(x^2)}{d x^2_i}\n",
    "\\end{equation}\n",
    "\n",
    "[NOTE:  The exponent on the $x$ indicates the second hidden layer.  That is, $x^2_i$ is the output of the $i$-th node in the second hidden layer. Also, recall that $\\hat{y}_i=\\sigma_i(x)$]\n",
    "\n",
    "(see Chain Rule: https://math.hmc.edu/calculus/hmc-mathematics-calculus-online-tutorials/multivariable-calculus/multi-variable-chain-rule/#:~:text=Multivariable%20Chain%20Rules%20allow%20us,ydydt .)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to derive a simple formula for this derivative.  From our previous work, for $i\\neq j$:\n",
    "\\begin{equation}\n",
    "\\frac{d \\sigma_j(x)}{d x_i} = -\\sigma_j(x)\\sigma_i(x)\n",
    "\\end{equation}\n",
    "and \n",
    "\\begin{equation}\n",
    "\\frac{d \\sigma_j(x)}{d x_j} = \\sigma_j(x) - \\sigma_j(x)^2\n",
    "\\end{equation}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the derivative of the cross entropy loss function with respect to the 10 inputs into the softmax: (recall $\\hat{y}_i=\\sigma_i(x^2)$)\n",
    "\\begin{equation}\n",
    "\\frac{d L(y,\\hat{y})}{d x^2_i}   =  \\sum_{j=1}^m \\frac{d L(y,\\hat{y})}{d \\hat{y}_j} \\frac{d \\hat{y}_j}{d x^2_i}    =   -\\sum_{j=1}^m \\frac{y_j}{\\hat{y}_j} \\frac{d \\sigma_j(x^2)}{d x^2_i} = -\\left( \\frac{y_1}{\\hat{y}_1}(-\\hat{y}_1\\hat{y}_i) + \\cdots +  \\frac{y_i}{\\hat{y}_i}( \\hat{y}_i-\\hat{y}_i^2) + \\cdots + \\frac{y_m}{\\hat{y}_m}(-\\hat{y}_m\\hat{y}_i)\\right)\n",
    "\\end{equation}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FINALLY, cancelling terms gives that the derivative of the cross entropy loss function with respect to the output of the $i$-th node of the second hidden layer is: \n",
    "\\begin{align}\n",
    "\\frac{d L(y,\\hat{y})}{d x^2_i} &= y_1\\hat{y}_i + \\cdots +  y_i( \\hat{y}_i-1) + \\cdots + y_m\\hat{y}_i \\\\\n",
    " &= y_1\\hat{y}_i + \\cdots +  y_i\\hat{y}_i + \\cdots + y_m\\hat{y}_i - y_i\n",
    "\\end{align}\n",
    "\n",
    "Since only $y_i$ equals 1 for the value of $i$ corresponding to the truth label and the rest are zero, letting $k$ be the truth value gives\n",
    "\\begin{equation}\n",
    "\\frac{d L(y,\\hat{y})}{d x^2_i} = \\hat{y}_i - y_i\n",
    "\\end{equation}\n",
    "\n",
    "ALSO SEE: https://davidbieber.com/snippets/2020-12-12-derivative-of-softmax-and-the-softmax-cross-entropy-loss/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derivative of Loss function with respect to the 10 outputs of the second hidden layer (x_0^2,...,x_9^2)\n",
    "def dL_dx2(network,obs,y):\n",
    "  yHat = network.apply(obs)\n",
    "  numx2i = network.layer2.numNodes \n",
    "  dLdx2 = np.zeros(numx2i)\n",
    "  k = np.where(y == 1)\n",
    "  for i in range(numx2i):\n",
    "    dLdx2[i] = yHat[i] - y[i]\n",
    "  return dLdx2\n",
    "\n",
    "idx = 0\n",
    "y = np.zeros(10)\n",
    "y[train_labels[idx]] = 1\n",
    "obs = train_images[idx,:,:].flatten()/255.\n",
    "print(dL_dx2(network,obs,y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the $i$-th node in the second layer is\n",
    "\\begin{equation}\n",
    "x^2_i = ReLU\\left( b^2_i + \\sum_{j=1}^{30}W^2_{i,j}x^1_j \\right) =\n",
    "\\begin{cases} \n",
    "      0 & \\textrm{if} & b^2_i + \\sum_{j=1}^{30}W^2_{i,j}x^1_j < 0 \\\\\n",
    "      b^2_i + \\sum_{j=1}^{30}W^2_{i,j}x^1_j & \\textrm{if} & b^2_i + \\sum_{j=1}^{30}W^2_{i,j}x^1_j > 0 \n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "The derivative of the output of the $i$-th node of layer 2 with respect to the weights of the second layer is:\n",
    "\\begin{equation}\n",
    "\\frac{d x^2_i}{dW^2_{i,j}} =\n",
    "\\begin{cases} \n",
    "      0 & \\textrm{if} & x^2_i = 0 \\\\\n",
    "      x^1_j & \\textrm{if} & x^2_i > 0 \n",
    "\\end{cases} \n",
    "\\end{equation}\n",
    "\n",
    "The derivative of the output of the $i$-th node of layer 2 with respect to the associated bias is:\n",
    "\\begin{equation}\n",
    "\\frac{d x^2_i}{db^2_i} =\n",
    "\\begin{cases} \n",
    "      0 & \\textrm{if} & x^2_i = 0 \\\\\n",
    "      1 & \\textrm{if} & x^2_i > 0 \n",
    "\\end{cases} \n",
    "\\end{equation}\n",
    "\n",
    "The derivative of the output of the $i$-th node of layer 2 with respect to the output of the $j$-th node of layer 1 is:\n",
    "\\begin{equation}\n",
    "\\frac{d x^2_i}{dx^1_j} =\n",
    "\\begin{cases} \n",
    "      0 & \\textrm{if} & x^2_i = 0 \\\\\n",
    "      W^2_{i,j} & \\textrm{if} & x^2_i > 0 \n",
    "\\end{cases} \n",
    "\\end{equation}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining the with previous results,\n",
    "\\begin{equation}\n",
    "\\frac{dL(y,\\hat{y})}{dW^2_{i,j}} = \\frac{d L(y,\\hat{y})}{d x^2_i}\\frac{d x^2_i}{dW^2_{i,j}} = \n",
    "\\begin{cases} \n",
    "      0 & \\textrm{if} & x^2_i = 0 \\\\\n",
    "      (\\hat{y}_i - y_i)x_j^1 & \\textrm{if} & x^2_i > 0 \n",
    "\\end{cases} \n",
    "\\end{equation}\n",
    "and\n",
    "\\begin{equation}\n",
    "\\frac{dL(y,\\hat{y})}{db^2_i} = \\frac{d L(y,\\hat{y})}{d x^2_i}\\frac{d x^2_i}{db^2_i} = \n",
    "\\begin{cases} \n",
    "      0 & \\textrm{if} & x^2_i = 0 \\\\\n",
    "      (\\hat{y}_i - y_i) & \\textrm{if} & x^2_i > 0 \n",
    "\\end{cases} \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dL_dW2(network,obs,y):\n",
    "  # the part in paranthesis (y1\\hat{y1},...,ym\\hat{ym})is dL_dx2:\n",
    "  dLdx2 = dL_dx2(network,obs,y)\n",
    "  numx2i = network.layer2.numNodes \n",
    "  numx1i = network.layer1.numNodes \n",
    "  # define the variable to hold the output \n",
    "  # this is an array the same size as the weight matrix\n",
    "  dLdW2 = np.zeros((numx2i,numx1i))\n",
    "  for i in range(numx2i):\n",
    "    for j in range(numx1i):\n",
    "      if (network.out2[i] == 0):\n",
    "        dLdW2[i,j] = 0\n",
    "      else:\n",
    "        dLdW2[i,j] = dLdx2[i]*network.out1[j]\n",
    "  return dLdW2\n",
    "\n",
    "def dL_db2(network,obs,y):\n",
    "  # the part in paranthesis (y1\\hat{y1},...,ym\\hat{ym})is dL_dx2:\n",
    "  dLdx2 = dL_dx2(network,obs,y)\n",
    "  numx2i = network.layer2.numNodes \n",
    "  # define the variable to hold the output \n",
    "  # this is a vector the same size as the number of nodes in layer 2\n",
    "  dLdb2 = np.zeros(numx2i)\n",
    "  for i in range(numx2i):\n",
    "    if (network.out2[i] == 0):\n",
    "      dLdb2[i] = 0\n",
    "    else:\n",
    "      dLdb2[i] = dLdx2[i]\n",
    "  return dLdb2\n",
    "\n",
    "def dx2_dx1(network,obs,y):\n",
    "  numx2i = network.layer2.numNodes \n",
    "  numx1i = network.layer1.numNodes \n",
    "  dx2dx1 = np.zeros((numx2i,numx1i))\n",
    "  for i in range(numx2i):\n",
    "    for j in range(numx1i):\n",
    "      if (network.out2[i] == 0):\n",
    "        dx2dx1[i,j] = 0\n",
    "      else:\n",
    "        dx2dx1[i,j] = network.layer2.weights[i,j]\n",
    "  return dx2dx1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "y = np.zeros(10)\n",
    "y[train_labels[idx]] = 1\n",
    "obs = train_images[idx,:,:].flatten()/255.\n",
    "\n",
    "dLdW2 = dL_dW2(network,obs,y)\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.suptitle('$dL / dW^2_{i,j}$', fontsize=20)\n",
    "plt.imshow(dLdW2);\n",
    "plt.xlabel('Weight ($j$)')\n",
    "plt.ylabel('Layer 2 Node ($i$)')\n",
    "plt.colorbar(orientation='vertical');\n",
    "\n",
    "dx2dx1 = dx2_dx1(network,obs,y)\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.suptitle('$dx_i^2 / dx_j^1$', fontsize=20)\n",
    "plt.imshow(dx2dx1);\n",
    "plt.xlabel('Layer 1 node ($j$)')\n",
    "plt.ylabel('Layer 2 node ($i$)')\n",
    "plt.colorbar(orientation='vertical');\n",
    "\n",
    "dLdb2 = dL_db2(network,obs,y)\n",
    "print('Derivative of L with respet to the biases in the second layer: '+str(dLdb2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2:  Compute formulas for the derivatives of the Loss function with respect to the parameters in the first layer, $W^1_{i,j}$ and $b^1_i$ for $i \\in \\{0,...,30\\}$ and $j \\in \\{0,...,784\\}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the second layer:\n",
    "\n",
    "The output of the $i$-th node in the first layer is\n",
    "\\begin{equation}\n",
    "x^1_i = ReLU\\left( b^1_i + \\sum_{j=1}^{30}W^1_{i,j}x_j \\right) =\n",
    "\\begin{cases} \n",
    "      0 & \\textrm{if} & b^1_i + \\sum_{j=1}^{30}W^1_{i,j}x_j < 0 \\\\\n",
    "      b^1_i + \\sum_{j=1}^{30}W^1_{i,j}x_j & \\textrm{if} & b^1_i + \\sum_{j=1}^{30}W^1_{i,j}x_j > 0 \n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "The derivative of the output of the $i$-th node of layer 1 with respect to the weights of the first layer is:\n",
    "\\begin{equation}\n",
    "\\frac{d x^1_i}{dW^1_{i,j}} =\n",
    "\\begin{cases} \n",
    "      0 & \\textrm{if} & x^1_i = 0 \\\\\n",
    "      x_j & \\textrm{if} & x^1_i > 0 \n",
    "\\end{cases} \n",
    "\\end{equation}\n",
    "and the derivative of the output of the $i$-th node of layer 2 with respect to the associated bias is:\n",
    "\\begin{equation}\n",
    "\\frac{d x^1_i}{db^1_i} =\n",
    "\\begin{cases} \n",
    "      0 & \\textrm{if} & x^1_i = 0 \\\\\n",
    "      1 & \\textrm{if} & x^1_i > 0 \n",
    "\\end{cases} \n",
    "\\end{equation}\n",
    "\n",
    "NOTE:  Note that he derivatives of the ReLU function are very simple.  This is one of the reasons we might choose this function, or a different function with similarly simple derivatives."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the chain rule,\n",
    "\\begin{equation}\n",
    "\\frac{dL(y,\\hat{y})}{dW^1_{i,j}} = \\sum_{k=0}^9 \\frac{d L(y,\\hat{y})}{d x^2_k}\\frac{d x^2_k}{d x^1_i}\\frac{d x^1_i}{dW^1_{i,j}} \n",
    "\\end{equation}\n",
    "\n",
    "From our previous work, the functions for the three derivatives in the chain rule are:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{d L(y,\\hat{y})}{d x^2_k} = \\hat{y}_k - y_k\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{d x^2_k}{dx^1_i} =\n",
    "\\begin{cases} \n",
    "      0 & \\textrm{if} & x^2_k = 0 \\\\\n",
    "      W^2_{k,i} & \\textrm{if} & x^2_k > 0 \n",
    "\\end{cases} \n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{d x^1_i}{dW^1_{i,j}} =\n",
    "\\begin{cases} \n",
    "      0 & \\textrm{if} & x^1_i = 0 \\\\\n",
    "      x_j & \\textrm{if} & x^1_i > 0 \n",
    "\\end{cases} \n",
    "\\end{equation}\n",
    "\n",
    "Multiplying these gives:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{dL(y,\\hat{y})}{dW^1_{i,j}}  =  \\sum_{k=0}^9 \n",
    "\\begin{cases} \n",
    "      0 & \\textrm{if } x^2_k = 0 \\textrm{ or } x^1_i = 0\\\\\n",
    "      (\\hat{y}_k - y_k) W^2_{k,i} x_j & \\textrm{otherwise}\n",
    "\\end{cases} \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dL_dW1(network,obs,y):\n",
    "  # the part in paranthesis (y1\\hat{y1},...,ym\\hat{ym})is dL_dx2:\n",
    "  dLdx2 = dL_dx2(network,obs,y)\n",
    "  numx2i = network.layer2.numNodes \n",
    "  numx1i = network.layer1.numNodes \n",
    "  numInputs = network.layer1.numInputs\n",
    "  # define the variable to hold the output \n",
    "  # this is an array the same size as the weight matrix\n",
    "  dLdW1 = np.zeros((numx1i,numInputs))\n",
    "  for i in range(numx1i):\n",
    "    for k in range(numx2i):\n",
    "      if not (network.out2[k] == 0) or (network.out1[i] == 0):\n",
    "        for j in range(numInputs):\n",
    "          dLdW1[i,j] = dLdW1[i,j] + dLdx2[k]*network.layer2.weights[k,i]*obs[j]\n",
    "  return dLdW1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the previous derivation: from the chain rule,\n",
    "\\begin{equation}\n",
    "\\frac{dL(y,\\hat{y})}{db^1_{i}} = \\sum_{k=0}^9 \\frac{d L(y,\\hat{y})}{d x^2_k}\\frac{d x^2_k}{d x^1_i}\\frac{d x^1_i}{b^1_{i}} \n",
    "\\end{equation}\n",
    "\n",
    "From our previous work, the functions for the three derivatives in the chain rule are:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{d L(y,\\hat{y})}{d x^2_k} = \\hat{y}_k - y_k\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{d x^2_k}{dx^1_i} =\n",
    "\\begin{cases} \n",
    "      0 & \\textrm{if} & x^2_k = 0 \\\\\n",
    "      W^2_{k,i} & \\textrm{if} & x^2_k > 0 \n",
    "\\end{cases} \n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{d x^1_i}{db^1_{i}} =\n",
    "\\begin{cases} \n",
    "      0 & \\textrm{if} & x^1_i = 0 \\\\\n",
    "      1 & \\textrm{if} & x^1_i > 0 \n",
    "\\end{cases} \n",
    "\\end{equation}\n",
    "\n",
    "Multiplying these gives:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{dL(y,\\hat{y})}{db^1_{i}}  =  \\sum_{k=0}^9 \n",
    "\\begin{cases} \n",
    "      0 & \\textrm{if } x^2_k = 0 \\textrm{ or } x^1_i = 0\\\\\n",
    "      (\\hat{y}_k - y_k) W^2_{k,i} & \\textrm{otherwise}\n",
    "\\end{cases} \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dL_db1(network,obs,y):\n",
    "  # the part in paranthesis (y1\\hat{y1},...,ym\\hat{ym})is dL_dx2:\n",
    "  dLdx2 = dL_dx2(network,obs,y)\n",
    "  numx2i = network.layer2.numNodes \n",
    "  numx1i = network.layer1.numNodes \n",
    "  numInputs = network.layer1.numInputs\n",
    "  # define the variable to hold the output \n",
    "  # this is an array the same size as the weight matrix\n",
    "  dLdb1 = np.zeros((numx1i))\n",
    "  for i in range(numx1i):\n",
    "    for k in range(numx2i):\n",
    "      if not (network.out2[k] == 0) or (network.out1[i] == 0):\n",
    "        dLdb1[i] = dLdb1[i] + dLdx2[k]*network.layer2.weights[k,i]\n",
    "  return dLdb1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2:  Create a method for the network that will use the derivatives to adjust the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the network (this time using the )\n",
    "class network2layers():\n",
    "  \n",
    "  def __init__(self, layer1, layer2, layerFinal):\n",
    "    self.layer1 = layer1\n",
    "    self.layer2 = layer2\n",
    "    self.layerFinal = layerFinal\n",
    "  \n",
    "  def apply(self, inputs):\n",
    "    self.inputs = inputs\n",
    "    # This determines the network 'architecture'\n",
    "    self.out1 = self.layer1.apply(self.inputs)\n",
    "    self.out2 = self.layer2.apply(self.out1)\n",
    "    self.outFinal = self.layerFinal(self.out2)\n",
    "    return self.outFinal\n",
    "\n",
    "  def train(self, obs, y, alpha):\n",
    "    #   obs = one observation (input into the network)\n",
    "    #   y = the truth value for the obs\n",
    "    #   alpha = the training rate\n",
    "    self.layer1.weights = self.layer1.weights - alpha*dL_dW1(network,obs,y)\n",
    "    self.layer1.biases = self.layer1.biases - alpha*dL_db1(network,obs,y)\n",
    "    self.layer2.weights = self.layer2.weights - alpha*dL_dW2(network,obs,y)\n",
    "    self.layer2.biases = self.layer2.biases - alpha*dL_db2(network,obs,y)\n",
    "    #print(dL_dW2(network,obs,y))\n",
    "\n",
    "# create an instance of this network \n",
    "# (the weights will be randomly selected since it is not trained)\n",
    "network = network2layers(layer1, layer2, layerFinal)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rounds of training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list to hold the loss as we train\n",
    "L = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_training_single = 0\n",
    "num_iters = 100\n",
    "probability_training = np.zeros(int(num_iters/20))\n",
    "for i in range(num_iters):\n",
    "  idx = random.randrange(num_train_images)\n",
    "  y = np.zeros(10)\n",
    "  y[train_labels[idx]] = 1\n",
    "  obs = train_images[idx,:,:].flatten()/255.  \n",
    "  network.train(obs, y, 0.0001)\n",
    "  \n",
    "  idx = random.randrange(num_test_images)\n",
    "  y = np.zeros(10)\n",
    "  y[test_labels[idx]] = 1\n",
    "  obs = test_images[idx,:,:].flatten()/255.  \n",
    "  yhat = network.apply(obs)\n",
    "  L.append(Loss(y,yhat))\n",
    "  probability_training_single = probability_training_single + yhat[test_labels[idx]]\n",
    "  if (i % 20) == 0:\n",
    "    print('Completed '+str(i)+' of '+str(num_iters))\n",
    "    print('Truth: '+str(test_labels[idx])+' | Predicted Prob: '+str(yhat[test_labels[idx]]))\n",
    "    print('Average Probability on Truth Class: '+str(probability_training_single/20)+' (above 0.1 is good, close to 1 is best) ')\n",
    "    probability_training[int(i/20)] = probability_training_single/20\n",
    "    probability_training_single = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(probability_training[1:])\n",
    "plt.xlabel('Iteration / 20')\n",
    "plt.ylabel('Probabliity for correct class')\n",
    "plt.title('Mean Probabilty for Correct Class (over groups of 20 iterations');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NNview(network.layer1, title='Node weights for first layer')\n",
    "NNview(network.layer2, title='Node weights for second layer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20\n",
    "plt.plot(L, label='Loss')\n",
    "plt.plot(np.convolve(L, np.ones(N)/N, mode='valid'), label='Sliding Mean')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NumCorrect = np.zeros(10)\n",
    "Num = np.zeros(10)\n",
    "\n",
    "for idx in range(2000):\n",
    "  idx = random.randrange(num_test_images)\n",
    "  y = np.zeros(10)\n",
    "  y[test_labels[idx]] = 1\n",
    "  obs = test_images[idx,:,:].flatten()/255.  \n",
    "  yhat = network.apply(obs)\n",
    "  Num[test_labels[idx]] = Num[test_labels[idx]] + 1\n",
    "  if (test_labels[idx] == np.argmax(yhat)):\n",
    "    NumCorrect[test_labels[idx]] = NumCorrect[test_labels[idx]] + 1\n",
    "\n",
    "print('Accuracy: '+str(np.sum(NumCorrect)/np.sum(Num)))\n",
    "accuracy = NumCorrect/Num\n",
    "bars = plt.bar(['0','1','2','3','4','5','6','7','8','9'], accuracy*100)\n",
    "plt.bar_label(bars);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add MiniBatch Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the network (this time using the )\n",
    "class network2layers():\n",
    "  \n",
    "  def __init__(self, layer1, layer2, layerFinal):\n",
    "    self.layer1 = layer1\n",
    "    self.layer2 = layer2\n",
    "    self.layerFinal = layerFinal\n",
    "  \n",
    "  def apply(self, inputs):\n",
    "    self.inputs = inputs\n",
    "    # This determines the network 'architecture'\n",
    "    self.out1 = self.layer1.apply(self.inputs)\n",
    "    self.out2 = self.layer2.apply(self.out1)\n",
    "    self.outFinal = self.layerFinal(self.out2)\n",
    "    return self.outFinal\n",
    "\n",
    "  def train(self, obs, y, alpha):\n",
    "    #   obs = one observation (input into the network)\n",
    "    #   y = the truth value for the obs\n",
    "    #   alpha = the training rate\n",
    "    self.layer1.weights = self.layer1.weights - alpha*dL_dW1(network,obs,y)\n",
    "    self.layer1.biases = self.layer1.biases - alpha*dL_db1(network,obs,y)\n",
    "    self.layer2.weights = self.layer2.weights - alpha*dL_dW2(network,obs,y)\n",
    "    self.layer2.biases = self.layer2.biases - alpha*dL_db2(network,obs,y)\n",
    "  \n",
    "  def trainMiniBatch(self, obs_list, y_list, alpha):\n",
    "    self.batch_size = len(y_list)\n",
    "    deltaL_dW1 = 0\n",
    "    deltaL_db1 = 0\n",
    "    deltaL_dW2 = 0\n",
    "    deltaL_db2 = 0\n",
    "    for i in range(self.batch_size):\n",
    "      y = y_list[i]\n",
    "      obs = obs_list[i]\n",
    "      deltaL_dW1 = deltaL_dW1 + dL_dW1(network,obs,y)\n",
    "      deltaL_db1 = deltaL_db1 + dL_db1(network,obs,y)\n",
    "      deltaL_dW2 = deltaL_dW2 + dL_dW2(network,obs,y)\n",
    "      deltaL_db2 = deltaL_db2 + dL_db2(network,obs,y)\n",
    "      #print(dL_dW1(network,obs,y))\n",
    "    self.layer1.weights = self.layer1.weights -  alpha*deltaL_dW1/self.batch_size\n",
    "    self.layer1.biases = self.layer1.biases - alpha*deltaL_db1/self.batch_size\n",
    "    self.layer2.weights = self.layer2.weights - alpha*deltaL_dW2/self.batch_size\n",
    "    self.layer2.biases = self.layer2.biases - alpha*deltaL_db2/self.batch_size\n",
    "    #print(deltaL_dW1)\n",
    "    #print(deltaL_db1)\n",
    "    #print(deltaL_dW2)\n",
    "    #print(deltaL_db2)\n",
    "\n",
    "# create an instance of this network \n",
    "# (the weights will be randomly selected since it is not trained)\n",
    "network = network2layers(layer1, layer2, layerFinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list to hold the loss as we train\n",
    "L = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iters = 20\n",
    "batch_size = 20\n",
    "for i in range(num_iters):\n",
    "  batch_idx = np.random.randint(0, high=num_train_images, size=batch_size)\n",
    "  y_list = []\n",
    "  obs_list = []\n",
    "  for idx in batch_idx:\n",
    "    y = np.zeros(10)\n",
    "    y[train_labels[idx]] = 1\n",
    "    y_list.append(y)\n",
    "    obs = train_images[idx,:,:].flatten()/255.  \n",
    "    obs_list.append(obs)\n",
    "  network.trainMiniBatch(obs_list, y_list, 1)\n",
    "  \n",
    "  idx = random.randrange(num_test_images)\n",
    "  y = np.zeros(10)\n",
    "  y[test_labels[idx]] = 1\n",
    "  obs = test_images[idx,:,:].flatten()/255.  \n",
    "  yhat = network.apply(obs)\n",
    "  L.append(Loss(y,yhat))\n",
    "  print('Completed '+str(i)+' of '+str(num_iters), end = '')\n",
    "  print(' -- Truth: '+str(test_labels[idx])+' | Predicted Prob: '+str(yhat[test_labels[idx]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "plt.plot(L, label='Loss')\n",
    "plt.plot(np.convolve(L, np.ones(N)/N, mode='valid'), label='Sliding Mean')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NNview(network.layer1, title='Node weights for first layer')\n",
    "plt.savefig('20k_iterations_node_weights1.png')\n",
    "NNview(network.layer2, title='Node weights for second layer')\n",
    "plt.savefig('20k_iterations_node_weights2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NumCorrect = np.zeros(10)\n",
    "Num = np.zeros(10)\n",
    "\n",
    "for idx in range(2000):\n",
    "  idx = random.randrange(num_test_images)\n",
    "  y = np.zeros(10)\n",
    "  y[test_labels[idx]] = 1\n",
    "  obs = test_images[idx,:,:].flatten()/255.  \n",
    "  yhat = network.apply(obs)\n",
    "  Num[test_labels[idx]] = Num[test_labels[idx]] + 1\n",
    "  if (test_labels[idx] == np.argmax(yhat)):\n",
    "    NumCorrect[test_labels[idx]] = NumCorrect[test_labels[idx]] + 1\n",
    "\n",
    "print('Accuracy: '+str(np.sum(NumCorrect)/np.sum(Num)))\n",
    "accuracy = NumCorrect/Num\n",
    "bars = plt.bar(['0','1','2','3','4','5','6','7','8','9'], accuracy*100)\n",
    "plt.bar_label(bars);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the network (this time using the )\n",
    "class network2layers():\n",
    "  \n",
    "  def __init__(self, layer1, layer2, layerFinal):\n",
    "    self.layer1 = layer1\n",
    "    self.layer2 = layer2\n",
    "    self.layerFinal = layerFinal\n",
    "  \n",
    "  def apply(self, inputs):\n",
    "    self.inputs = inputs\n",
    "    # This determines the network 'architecture'\n",
    "    self.out1 = self.layer1.apply(self.inputs)\n",
    "    self.out2 = self.layer2.apply(self.out1)\n",
    "    self.outFinal = self.layerFinal(self.out2)\n",
    "    return self.outFinal\n",
    "\n",
    "  def train(self, obs, y, alpha):\n",
    "    #   obs = one observation (input into the network)\n",
    "    #   y = the truth value for the obs\n",
    "    #   alpha = the training rate\n",
    "    self.layer1.weights = self.layer1.weights - alpha*dL_dW1(network,obs,y)\n",
    "    self.layer1.biases = self.layer1.biases - alpha*dL_db1(network,obs,y)\n",
    "    self.layer2.weights = self.layer2.weights - alpha*dL_dW2(network,obs,y)\n",
    "    self.layer2.biases = self.layer2.biases - alpha*dL_db2(network,obs,y)\n",
    "  \n",
    "  def trainMiniBatch(self, obs_list, y_list, alpha, w_decay):\n",
    "    self.batch_size = len(y_list)\n",
    "    deltaL_dW1 = 0\n",
    "    deltaL_db1 = 0\n",
    "    deltaL_dW2 = 0\n",
    "    deltaL_db2 = 0\n",
    "    for i in range(self.batch_size):\n",
    "      y = y_list[i]\n",
    "      obs = obs_list[i]\n",
    "      deltaL_dW1 = deltaL_dW1 + dL_dW1(network,obs,y)\n",
    "      deltaL_db1 = deltaL_db1 + dL_db1(network,obs,y)\n",
    "      deltaL_dW2 = deltaL_dW2 + dL_dW2(network,obs,y)\n",
    "      deltaL_db2 = deltaL_db2 + dL_db2(network,obs,y)\n",
    "      #print(dL_dW1(network,obs,y))\n",
    "    self.layer1.weights = self.layer1.weights - alpha*(w_decay*np.abs(self.layer1.weights) + deltaL_dW1/self.batch_size)\n",
    "    self.layer1.biases = self.layer1.biases - alpha*(w_decay*np.abs(self.layer1.biases) + deltaL_db1/self.batch_size)\n",
    "    self.layer2.weights = self.layer2.weights - alpha*(w_decay*np.abs(self.layer2.weights) + deltaL_dW2/self.batch_size)\n",
    "    self.layer2.biases = self.layer2.biases - alpha*(w_decay*np.abs(self.layer2.biases) + deltaL_db2/self.batch_size)\n",
    "    #print(deltaL_dW1)\n",
    "    #print(deltaL_db1)\n",
    "    #print(deltaL_dW2)\n",
    "    #print(deltaL_db2)\n",
    "\n",
    "# create an instance of this network \n",
    "# (the weights will be randomly selected since it is not trained)\n",
    "network = network2layers(layer1, layer2, layerFinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iters = 200\n",
    "batch_size = 20\n",
    "for i in range(num_iters):\n",
    "  batch_idx = np.random.randint(0, high=num_train_images, size=batch_size)\n",
    "  y_list = []\n",
    "  obs_list = []\n",
    "  for idx in batch_idx:\n",
    "    y = np.zeros(10)\n",
    "    y[train_labels[idx]] = 1\n",
    "    y_list.append(y)\n",
    "    obs = train_images[idx,:,:].flatten()/255.  \n",
    "    obs_list.append(obs)\n",
    "  network.trainMiniBatch(obs_list, y_list, 1, 0.01)\n",
    "  \n",
    "  idx = random.randrange(num_test_images)\n",
    "  y = np.zeros(10)\n",
    "  y[test_labels[idx]] = 1\n",
    "  obs = test_images[idx,:,:].flatten()/255.  \n",
    "  yhat = network.apply(obs)\n",
    "  L.append(Loss(y,yhat))\n",
    "  print('Completed '+str(i)+' of '+str(num_iters), end = '')\n",
    "  print(' -- Truth: '+str(test_labels[idx])+' | Predicted Prob: '+str(yhat[test_labels[idx]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "plt.plot(L, label='Loss')\n",
    "plt.plot(np.convolve(L, np.ones(N)/N, mode='valid'), label='Sliding Mean')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NNview(network.layer1, title='Node weights for first layer')\n",
    "plt.savefig('20k_iterations_node_weights1.png')\n",
    "NNview(network.layer2, title='Node weights for second layer')\n",
    "plt.savefig('20k_iterations_node_weights2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NumCorrect = np.zeros(10)\n",
    "Num = np.zeros(10)\n",
    "\n",
    "for idx in range(2000):\n",
    "  idx = random.randrange(num_test_images)\n",
    "  y = np.zeros(10)\n",
    "  y[test_labels[idx]] = 1\n",
    "  obs = test_images[idx,:,:].flatten()/255.  \n",
    "  yhat = network.apply(obs)\n",
    "  Num[test_labels[idx]] = Num[test_labels[idx]] + 1\n",
    "  if (test_labels[idx] == np.argmax(yhat)):\n",
    "    NumCorrect[test_labels[idx]] = NumCorrect[test_labels[idx]] + 1\n",
    "\n",
    "print('Accuracy: '+str(np.sum(NumCorrect)/np.sum(Num)))\n",
    "accuracy = NumCorrect/Num\n",
    "bars = plt.bar(['0','1','2','3','4','5','6','7','8','9'], accuracy*100)\n",
    "plt.bar_label(bars);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
