---
title: "DS-6030 Homework Module 2"
author: "Tom Lever"
date: 06/03/2023
output:
  pdf_document: default
  html_document: default
urlcolor: blue
---

<!--- Below are global settings for knitr. You can override any of them by adding the changes to individual chunks --->
```{r global_options, include = FALSE}
knitr::opts_chunk$set(
    error = TRUE, # Keep compiling upon error
    collapse = FALSE, # code and corresponding output appear in knit file in separate blocks
    echo = TRUE, # echo code by default
    comment = "#", # change comment character
    #fig.width = 5.5, # set figure width
    fig.align = "center", # set figure position
    #out.width = "49%", # set width of displayed images
    warning = TRUE, # do not show R warnings
    message = TRUE # do not show R messages
)
```

<!--- Change font size for headers --->
<!--
<style>
    h1.title {
        font-size: 28px;
    }
    h1 {
        font-size: 22px;
    }
    h2 {
        font-size: 18px;
    }
    h3 {
        font-size: 14px;
    }
</style>
-->

**DS 6030 | Spring 2022 | University of Virginia **

1.  This question involves the use of multiple linear regression on the Auto data set.

    (a) Produce a scatterplot matrix which includes all of the variables in the data set.
    
        ```{r}
        library(ISLR2)
        pairs(Auto, main = "Scatterplot Matrix Of Auto")
        ```

    (b) Compute the matrix of correlations between the variables using the function `cor()`. You will need to exclude the name variable, which is qualitative.
    
        We randomize the order of the rows of our data set so that AutoCorrelation Function (ACF) values for nonzero lags for a multiple linear regression model are insignificant, and so that an assumption that the residuals of our model are uncorrelated is met.
    
        ```{r}
        library(TomLeversRPackage)
        index_of_column_name <- get_index_of_column_of_data_frame(Auto, "name")
        data_frame_of_columns_except_name <- Auto[, -index_of_column_name]
        set.seed(0)
        number_of_rows <- nrow(data_frame_of_columns_except_name)
        vector_of_random_row_indices <- sample(1:number_of_rows)
        data_frame_of_columns_except_name <-
            data_frame_of_columns_except_name[vector_of_random_row_indices, ]
        correlation_matrix <- cor(data_frame_of_columns_except_name)
        correlation_matrix
        ```

    (c) Use the `lm()` function to perform a multiple linear regression with `mpg` as the response and all other variables except name as the predictors. Use the `summary()` function to print the results. Comment on the output. For instance:
    
        ```{r}
        linear_model <- lm(
            formula = mpg ~ cylinders + displacement + horsepower + weight + acceleration + year + origin,
            data = data_frame_of_columns_except_name
        )
        summarize_linear_model(linear_model)
        ```

        i.  Is there a relationship between the predictors and the response?
        
            ```{r}
            analyze_variance_for_one_linear_model(linear_model)
            ```
            
            We assume that the errors for the above linear model are random, are independent, and follow a normal distribution with mean $E\left(\epsilon_i\right) = 0$ and variance $Var\left(\epsilon_i\right) = \sigma^2$. There is a relationship between the predictors and the response if at least one of the predictor variables in the set $\{cylinders, displacement, horsepower, weight, acceleration, year, origin\}$ contributes significantly to the model. We conduct a test of the null hypothesis $H_0: \beta_{cylinders} = \beta_{displacement} = \beta_{horsepower} = \beta_{weight} = \beta_{acceleration} = \beta_{year} = \beta_{origin} = 0$ that all coefficients in the set $\{\beta_{cylinders}, \beta_{displacement}, \beta_{horsepower}, \beta_{weight}, \beta_{acceleration}, \beta_{year}, \beta_{origin}\}$ are $0$. The alternate hypothesis is $H_1: \beta_{cylinders} \neq 0 \ or \ \beta_{displacement} \neq 0 \ or \ \beta_{horsepower} \neq 0 \ or \ \beta_{weight} \neq 0 \ or \ \beta_{acceleration} \neq 0 \ or \ \beta_{year} \neq 0 \ or \ \beta_{origin} \neq 0$ that at least one coefficient in the set $\{\beta_{cylinders}, \beta_{displacement}, \beta_{horsepower}, \beta_{weight}, \beta_{acceleration}, \beta_{year}, \beta_{origin}\}$ is not $0$. The alternate hypothesis is also $H_1: \beta_i \neq 0 \ for \ i \in \{\beta_{cylinders}, \beta_{displacement}, \beta_{horsepower}, \beta_{weight}, \beta_{acceleration}, \beta_{year}, \beta_{origin}\}$. If we reject the null hypothesis, at least one of the predictor variables contributes significantly to the model.
            
            ```{r}
            test_null_hypothesis_involving_MLR_coefficients(linear_model, 0.05)
            ```
            
            Alternately, since the test statistic $F_0 = 252.428$ is greater than the critical value $F_{\alpha = 0.05, \ df_R = p = 7, \ df_{Res} = n - v = 392 - 8} = 2.033$, we reject the null hypothesis and support the alternate hypothesis. Since we reject the null hypothesis and support the alternate hypothesis, at least one of the predictors contributes significantly to the model. Since at least one of the predictors contributes significantly to the model, there a relationship between the predictors and the response.
        
        ii. Which predictors appear to have a statistically significant relationship to the response?
        
            A critical value $t_{alpha/2 = 0.05, \ n - v = 392 - 8} = 1.966$. The summary for the above linear model provides test statistics for predictors. In parallel, the summary provides probabilities where each probability $p$ is the probability that the magnitude $|t|$ of a random test statistic is greater than the magnitude $|t_0|$ of the appropriate test statistic. Because the magnitudes of the test statistic for displacement, weight, year, and origin are greater than the critical value, and the probabilities for these predictors are less than the significance level $\alpha = 0.05$, we reject null hypotheses that each individual predictor is insignificant in predicting the response in the context of the model and can be removed from the model. For these predictors we have sufficient evidence to support the alternate hypothesis that the predictor is significant in predicting the response in the context of the model and cannot be removed from the model.
        
        iii. What does the coefficient for the year variable suggest?
        
             The coefficient for predictor year $\beta_{year} = 0.751$ suggests that for every increase of $1$ year, response fuel efficiency $mpg$ increases by $0.751$, all other predictors being equal.

    (d) Use the `plot()` function to produce diagnostic plots of the linear regression fit. Comment on any problems you see with the fit. Do the residual plots suggest any unusually large outliers? Does the leverage plot identify any observations with unusually high leverage?
    
        ```{r, echo = FALSE, eval = FALSE}
        library(ggplot2)
        ggplot(
            data.frame(
                residual = linear_model$residuals,
                predicted_mpg = linear_model$fitted.values
            ),
            aes(x = predicted_mpg, y = residual)
        ) +
        geom_point(alpha = 0.2) +
        geom_hline(yintercept = 0, color = "red") +
        labs(
            x = "predicted_mpg",
            y = "residual",
            title = "Residuals vs. Predicted MPG"
        ) +
        theme(
            plot.title = element_text(hjust = 0.5),
            axis.text.x = element_text(angle = 0)
        )
        ```
        
        ```{r}
        plot(linear_model)
        ```
        
        We use the Student's $t$ distribution and the Bonferroni procedure to find a cutoff value for outlier detection using externally studentized residuals. Let $n = 101$ be the number of observations and let $df_{Res}$ be the number of degrees of freedom for our recommended model. If magnitude of externally studentized residual $|t_i|$ is greater than a critical value $t_{\alpha/(2n), df_{Res} - 1}$, observation $i$ is deemd an outlier. Our new observation is an outlier.
        
        A leverage $h_{ii}$ is used to identify how far observation $i$ is from the centroid of the predictor space. If leverage $h_{ii} > \frac{2p}{n}$, then observation $i$ is deemed to have high leverage and is outlying in the predictor space. High leverage observations are data points that are most likely to be influential. There are $32$ outliers and $17$ high-leverage observations.
        
        ```{r}
        significance_level <- 0.05
        number_of_observations <- get_number_of_observations(linear_model)
        residual_degrees_of_freedom <-
            calculate_residual_degrees_of_freedom(linear_model)
        critical_value_tc <- calculate_critical_value_tc(
            significance_level,
            number_of_observations,
            residual_degrees_of_freedom - 1,
            hypothesis_test_is_two_tailed = FALSE
        )
        externally_studentized_residuals <- rstudent(linear_model)
        vector_of_outliers <- externally_studentized_residuals[
            abs(externally_studentized_residuals) > critical_value_tc
        ]
        length(vector_of_outliers)
        leverages <- lm.influence(linear_model)$hat
        number_of_variables <- get_number_of_variables(linear_model)
        high_leverages <-
            leverages[leverages > 2 * number_of_variables / number_of_observations]
        indices_of_high_leverage_observations <- as.numeric(names(high_leverages))
        length(indices_of_high_leverage_observations)
        ```
        
        According to [https://data.library.virginia.edu/diagnostic-plots/](https://data.library.virginia.edu/diagnostic-plots/), a plot of residuals vs. fitted values shows if residuals have non-linear patterns. There could be a non-linear relationship between predictor variables and an outcome variable, and the pattern could show up in this plot if the model doesn't capture the non-linear relationship. If you find equally spread residuals around a horizontal line without distinct patterns, that is a good indication you don't have non-linear relationships."
        
        Our plot of residuals vs. fitted values exhibits a "U" shape. According to [https://mathstat.slu.edu/~speegle/_book_summer_2021/SimpleReg.html](https://mathstat.slu.edu/~speegle/_book_summer_2021/SimpleReg.html), "A pattern such as a U-shape is evidence of a lurking variable that we have not taken into account. A lurking variable could be information related to the data that we did not collect, or it could be that our model should include a quadratic term."
        
        According to [https://data.library.virginia.edu/diagnostic-plots/](https://data.library.virginia.edu/diagnostic-plots/), a normal Q-Q plot "shows if residuals are normally distributed. Do residuals follow a straight line well or do they deviate severely? It's good if residuals are lined well on the straight dashed line." According to [https://data.library.virginia.edu/understanding-q-q-plots/](https://data.library.virginia.edu/understanding-q-q-plots/), the QQ plot, or quantile-quantile plot, is a graphical tool to help us assess if a set of data plausibly came from some theoretical distribution such as a normal or exponential. For example, if we run a statistical analysis that assumes our residuals are normally distributed, we can use a normal QQ plot to check that assumption. It's just a visual check, not an air-tight proof, so it is somewhat subjective. But it allows us to see at-a-glance if our assumption is plausible, and if not, how the assumption is violated and what data points contribute to the violation."
        
        "A QQ plot is a scatterplot created by plotting two sets of quantiles against one another. If both sets of quantiles came from the same distribution, we should see the points forming a line that's roughly straight."
        
        "Now what are 'quantiles'? These are often referred to as 'percentiles'. These are points in your data below which a certain proportion of your data fall. For example, imagine the classic bell-curve standard normal distribution with a mean of $0$. The $0.5$ quantile, or $50$th percentile, is $0$. Half the data lie below $0$. That's thge peak of the hump in the curve. The $0.95$ quantile, or $95$th percentile, is about $1.64$. $95$ percent of the data lie below $1.64$."
        
        "So we see that quantiles are basically just your data sorted in ascending order, with various data points labelled as being the point below which a certain proportion of the data fall."
        
        QQ plots plot standardized residuals "versus quantiles calculated from a theoretical distribution. The number is quantiles is selected to match the size of your sample data. While normal QQ plots are the ones most often used in practice due to so many statistical methods assuming normality, QQ plots can actually be created for any distribution."
        
        "What about when points don't fall on a straight line? What can we infer about our data?" A QQ plot for which the upper right points deviate up from the line of the QQ plot indicates that your sample distribution has "heavy tails", which means that "your data has a larger number of extreme values than would be expected if they truly came from a normal distribution."
        
        According to [https://data.library.virginia.edu/diagnostic-plots/](https://data.library.virginia.edu/diagnostic-plots/), a Scale / Spread - Location plot "shows if residuals are spread equally along the ranges of predictors. This is how you can check the assumption of equal variance (homoscedasticity). It's good if you see a horizontal line with equally (randomly) spread points." Residuals exhibit a "U" shape indicating that the multiple linear regression assumption that the variance of errors is constant is not met.
        
        A plot of standardized residuals vs. leverage "helps us find influential cases (i.e., subjects) if there are any. Not all outliers are influential in linear regression analysis (whatever outliers mean). Even though data have extreme values, they might not be influential to determine a regression line. THat means the results wouldn't be much different if we either include or exclude them from analysis. They follow the trend in the majority of cases and they don't really matter; the are not influential. On the other hand, some cases could be very influential even if they look to be within a reasonable range of the values. They could be extreme cases against a regression line and can alter the results if we exclude them from analysis. Another way to put it is that they don't get along with the trend in the majority of the cases."
        
        "Unlike the other plots, this time patterns are not relevant. We watch out for outlying values at the upper right corner or at the lower right corner. Those spots are the places where cases can be influential against a regression line. Look for cases outside of the dashed lines. When cases are outside of the dashed lines (meaning they have high 'Cook's distance' scores), the cases are influential to the regression results. The regression results will be altered if we exclude those cases."
        
        In our case where you "can barely see Cook's distance lines" and "all cases are well inside of the Cook's distance lines", there are no influential cases. In a case where a point "is far beyond the Cook's distance lines", the point corresponds to an influential observation.
        
        Our plot of standardized residuals vs. leverage shows a point corresponding to observation 14 with relatively high leverage.
        
        "The four plots show potential problematic cases with the row numbers of the cases in the data set. If some cases are identified across all four plots, you might want to take a closer look at them individually. Is there anything special for the subject? Or could it be simply errors in data entry?" Observations 323, 326, and 327 are potentially problematic or outliers according to the plot of residuals vs. fitted values, the plot of standardized residuals vs. theoretical quantiles, and the Scale / Spread - Location plot. Observation 327 additionally is identified as potentially problematic or an outlier in the plot of standardized residuals vs. leverage. If we define outlier to being observation with a magnitude of standardized residual greater than 2, the plot of standardized residuals vs. leverage shows some outliers.
        
        "In that case, you may want to go back to your theory and hypotheses. Is it really a linear relationship between the predictors and the outcome? You may want to include a quadratic term, for example. A log transformation may better represent the phenomena that you'd like to model. Or, is there any important variable that you left out from your model? Other variables you didn't include (e.g., age or gender) may play an important role in your model and data. Or, maybe, your data were systematically biased when collecting data. You may want to redesign data collection methods."

    (e) Use the `*` and `:` symbols to fit linear regression models with interaction effects. Do any interactions appear to be statistically significant?
    
        Cylinders and displacement have the highest correlation at $0.951$. Displacement and weight have the second highest correlation at $0.932$. Let us consider a linear model involving cylinders, displacement, and a term representing interaction between cylinders and displacement. Let us consider a linear model involving displacement, weight, and a term representing interaction between displacement and weight. Let us consider a linear model involving cylinders, displacement, weight, an interaction term involving cylinders and displacement, and an interaction term involving displacement and weight.
        
        For the linear model $mpg = \beta_0 + \beta_1 \ cylinders + \beta_2 \ displacement + \beta_3 \ cylinders \ displacement$, since the $p$ value for each predictive term is less than significance level $0.05$, each predictive term is statistically significant.
        
        For the linear model $mpg = \beta_0 + \beta_1 \ displacement + \beta_2 \ weight + \beta_3 \ displacement \ weight$, since the $p$ value for each predictive term is less than significance level $0.05$, each predictive term is statistically significant.
        
        For the linear model $mpg = \beta_0 + \beta_1 \ cylinders + \beta_2 \ displacement + \beta_3 \ weight + \beta_4 \ cylinders \ displacement + \beta_5 \ displacement \ weight$, since the $p$ value for the predictive term $displacement \ weight$ is less than significance level $0.05$, this predictive term is statistically significant. Since the $p$ value for the predictive term $cylinders \ displacement$ is greater than significance level $0.05$, each predictive term is statistically insignificant.
    
        ```{r}
        linear_model <- lm(
            formula = mpg ~ cylinders * displacement,
            data = data_frame_of_columns_except_name
        )
        summarize_linear_model(linear_model)
        linear_model <- lm(
            formula = mpg ~ displacement + weight + displacement : weight,
            data = data_frame_of_columns_except_name
        )
        summarize_linear_model(linear_model)
        linear_model <- lm(
            mpg ~ cylinders * displacement + displacement * weight,
            data = data_frame_of_columns_except_name
        )
        summarize_linear_model(linear_model)
        ```

    (f) Try a few different transformations of the variables, such as $\log(X)$, $\sqrt{X}$, $X^2$. Comment on your findings.
    
        We study fuel efficiency by constructing a multiple linear regression model. Below, we describe a process to determine a recommended multiple linear regression model.
        
        We consider $p = 7$ simple linear regression models, each with response $mpg$ and one of the $p = 7$ predictors. For each simple linear regression model, we meet simple linear regression assumptions that 1) the relationship between response and predictor is linear, 3) the variance of residuals is constant, and 2) the mean residual is $0$. To meet these assumptions, we examine residual plots and apply transformations.
        
        Specifically, we consider the plot of residuals vs. predicted values for each simple linear regression model. The residual plots for predictors $cylinders$, $displacement$, $horsepower$, and $weight$ exhibits a right-opening, positively biased funnel shape suggesting that the residuals of the appropriate simple linear regression model have variance that increases with $mpg$ and mean residual greater $0$. The residual plots for the simple linear regression models with predictors $displacement$, $horsepower$, $weight$, and $origin$ additionally offer an impression of a "U" shaped band, suggesting that the relationship between $mpg$ and predictor may be nonlinear.
        
        We consider the Box Cox maximum likelihood estimate of parameter $\lambda$ for each simple linear regression model. The Box Cox maximum likelihood estimate of parameter $\lambda$ for predictors $cylinders$, $displacement$, $horsepower$, $weight$, $acceleration$, and $year$ are $-0.293$, $-0.354$, $-0.475$, $-0.333$, $0.091$, and $0.253$. We create simple linear regression models with response $mpg^{-0.5}$ for each predictor $cylinders$, $displacement$, $horsepower$, and $weight$. We create a simple linear regression model with response $ln(mpg)$ for predictor $acceleration$. We create a simple linear regression model with response $sqrt(mpg)$ for predictor $year$. For each simple linear regression model with transformed response, we consider a plot of residuals versus predictor values.
        
        Following *Introduction to Linear Regression Analysis* (Sixth Edition) by Douglas C. Montgomery et al., for each plot of residuals versus predictor values, an impression of a horizontal band containing the residuals, centered at $e = 0$, is desirable. A nonlinear pattern in general implies that the assumed relationship between a transformed response and the predictor is not correct. A curved band that looks quartic suggests a transformation $x' = x^4$. The plots of residuals versus predictor for predictors $displacement$, $horsepower$, $weight$, $acceleration$, $year$, and $origin$ offer an impression of a horizontal band. The plot of residuals versus predictor for predictor $cylinders$ has a curved band that looks quartic. We apply the transformation $x' = x^4$ to $cylinders$.
        
        It is not helpful to create a simple linear model with response $sqrt(origin)$ even when we subsequently transform $origin$ according to $origin' = origin^2$.
        
        We consider the residual plot for $mpg^{-0.5} = cylinders^4$ and the residual plots for each simple linear regression model not on $cylinders$ with transformed response. These simple linear regression models offer the impression of a horizontal band centered at $e = 0$. The above simple linear regression assumptions are met for these simple linear regression models. As a reminder, these simple linear regression assumptions are 1) the relationship between response and predictor is linear, 3) the variance of residuals is constant, and 2) the mean residual is $0$.
        
        We present below the plots of AutoCorrelation Function Value vs. Lag for the simple linear regression model $mpg^{-0.5} = cylinders^4$ and the other simple linear regression models not on $cylinders$ with transformed response. Most AutoCorrelation Function values are insignificant for these models. The remaining AutoCorrelation Function values for these models are approximately insignificant. A simple linear regression assumption that residuals are uncorrelated is met approximately for these models.
        
        Additionally, we consider the QQ plots for the simple linear regression model $mpg^{-0.5} = cylinders^4$ and the other simple linear regression models not on $cylinders$ with transformed response. The distribution of quantiles for $mpg^{-0.5} = cylinders^4$ is normal. The distribution of quantiles for $displacement$ and $origin$ are skewed right. The distribution of quantiles for $horsepower$ is mildly lightly tailed. The distribution of quantiles for $weight$ is lightly tailed. The distribution of quantiles for $acceleration$ and $year$ are heavy tailed. A simple linear regression assumption that residuals are normally distributed is approximately met for these simple linear regression models. Simple linear regression is robust to this assumption.
        
        After analyzing the correlation matrix for transformed predictors, because $cylinders^4$ and $displacement$ are at least moderately correlated with all other predictors other than $year$, we choose as our working model a simple linear regression model on either $cylinders$ or $displacement$. Specifically, we choose the model with lower Akaike Information Criterion (AIC):
        $$mpg^{-0.5} = \beta_0 + \beta_1 \ displacement$$
        We can consider studying fuel efficiency with our working model. Alternatively, since we considered simple linear regression model $mpg^{0.5} = \beta_0 + \beta_1 \ year$ and the response of our working model is $mpg^{-0.5}$, we can consider a multiple linear regression model with predictor $year^{-1}$. Based on the output of R's forward-selection function, we determine that a multiple linhear regression model with $year^{-1}$ has a lower AIC than our working model. We choose as our working multiple linear regression model for studying fuel efficiency:
        $$mpg^{-0.5} = \beta_0 + \beta_1 \ displacement + \beta_2 \ year^{-1}$$
        Following "Detecting Multicollinearity Using Variance Inflation Factors" ([https://online.stat.psu.edu/stat462/node/180/](https://online.stat.psu.edu/stat462/node/180/)), the variance inflation factor $VIF_j$ corresponding to predictor $x_j$ in a multiple linear regression model quantifies how much the variance of the estimated coefficient $\beta_j$ corresponding to predictor $x_j$ is inflated due to multicollinearity / correlation between predictor $x_j$ and other predictors. In particular, the variance inflation factor for predictor $x_j$
        $$VIF_j = \frac{1}{1 - {R_j}^2}$$
        where ${R_j}^2$ is the coefficient of determination obtained by regressing predictor $x_j$ on the remaining predictors. A VIF of 1 means that there is no correlation among predictor $x_j$ and the remaining predictors and that the variance of the regression coefficient corresponding to predictor $x_j$ is not inflated. The general rule of thumb is that VIF's exceeding $4$ warrant further investigation, while VIF's exceeding $10$ are signs of serious multicollinearity requiring correction.
        
        The Variance Inflation Factors for our working multiple linear regression model are both $1.141$, suggesting that multicollinearity between $cylinders^4$ and $year^{-1}$ is acceptable.
        
        Summarizing, above we assessed linear regression models by residual analysis, correlation analysis, AIC, forward selection, and multicollinearity. Through residual analysis, correlation analysis, and AIC, we chose intermediary working model $mpg^{0.5} = \beta_0 + \beta_1 \ cylinders^4$. Through forward selection we added $year^{-1}$. We recommend our working multiple linear regression model
        $$mpg^{-0.5} = \beta_0 + \beta_1 \ displacement + \beta_2 \ year^{-1}$$
        For our recommended multiple linear regression model, a plot of residuals versus predicted values, a QQ plot, a Scale / Spread - Location plot, a plot of standardized residuals versus leverage, and a plot of AutoCorrelation Function values vs. lag are presented below.
        Assumptions for this multiple linear regression model are met.
        * The relationship between fuel efficiency $mpg^{-0.5}$, $cylinders^4$, and $year^{-1}$ is linear; the residuals are evenly scattered across $e = 0$.
        * The mean residual is $0$; the residuals are evenly scattered across $e = 0$.
        * The variance of the residuals is constant; the residuals are evenly spread for different predicted values.
        * The residuals are uncorrelated; AutoCorrelation Function values are insignificant.
        * The distribution of residuals is mildly lightly tailed. The residuals are approximately normally distributed.
        
        Using the summary of our recommended model above, we conduct an ANOVA $F$ Test / Partial $F$ Test involving all predictors. The null and alternate hypothesis for this test are $H_0: \boldsymbol{\beta} = \boldsymbol{0}$ and $H_1: \boldsymbol{\beta} \neq 0$. A test statistic $F_0 = 614.7$ is greater than a critical value $F_{\alpha = 0.05, \ df_R = 2, \ df_{Res} = 389} = 3.019$. We reject the null hypothesis that all regression coefficients are $0$ and prefer our recommended multiple linear regression model to an intercept only model.
        
        Since each test statistic $t$ in the summary of our recommended model is greater than a critical value $t_{\alpha/2 = 0.05/2, \ df_{Res} = 389} = 1.649$, for each corresponding predictor, we reject a null hypothesis that the regression coefficient for that predictor is $0$, and conclude that each predictor is significant in the context of the multiple linear regression model / all predictors.
        
        Since the multiple and adjusted coefficients of determination $R^2$ for our recommended model are $0.760$ and $0.758$, a high proportion of variation in $mpg^{-0.5}$ can be explained by the predictors $cylinders^4$ and $year^{-1}$. However, because these coefficients are both less than a common threshold of $0.8$, our linear model may not be good for prediction.

        ```{r, echo = FALSE, eval = FALSE}
        # full model: mpg ~ cylinders + displacement + horsepower + weight + acceleration + year + origin
        linear_model <- lm(
            formula = mpg ~ cylinders,
            data = data_frame_of_columns_except_name
        )
        generate_residual_plot(
            linear_model = linear_model,
            should_plot_residuals_versus_predicted_values = TRUE
        )
        perform_Box_Cox_Method(
            linear_model,
            vector_of_values_of_lambda = seq(-1, 1, 0.1),
            whether_to_plot = TRUE
        )
        linear_model <- lm(
            formula = I(mpg^{-1/2}) ~ cylinders,
            data = data_frame_of_columns_except_name
        )
        generate_residual_plot(
            linear_model = linear_model,
            should_plot_residuals_versus_predicted_values = TRUE
        )
        linear_model <- lm(
            formula = I(mpg^{-1/2}) ~ I(cylinders^4),
            data = data_frame_of_columns_except_name
        )
        generate_residual_plot(
            linear_model = linear_model,
            should_plot_residuals_versus_predicted_values = TRUE
        )
        acf(linear_model$residuals)
        qqnorm(linear_model$residuals)
        qqline(linear_model$residuals, col = "red")
        linear_model <- lm(
            formula = mpg ~ displacement,
            data = data_frame_of_columns_except_name
        )
        generate_residual_plot(
            linear_model = linear_model,
            should_plot_residuals_versus_predicted_values = TRUE
        )
        perform_Box_Cox_Method(
            linear_model,
            vector_of_values_of_lambda = seq(-1, 1, 0.1),
            whether_to_plot = TRUE
        )
        linear_model <- lm(
            formula = I(mpg^{-1/2}) ~ displacement,
            data = data_frame_of_columns_except_name
        )
        generate_residual_plot(
            linear_model = linear_model,
            should_plot_residuals_versus_predicted_values = TRUE
        )
        acf(linear_model$residuals)
        qqnorm(linear_model$residuals)
        qqline(linear_model$residuals, col = "red")
        linear_model <- lm(
            formula = mpg ~ horsepower,
            data = data_frame_of_columns_except_name
        )
        generate_residual_plot(
            linear_model = linear_model,
            should_plot_residuals_versus_predicted_values = TRUE
        )
        perform_Box_Cox_Method(
            linear_model,
            vector_of_values_of_lambda = seq(-1, 1, 0.1),
            whether_to_plot = TRUE
        )
        linear_model <- lm(
            formula = I(mpg^{-1/2}) ~ horsepower,
            data = data_frame_of_columns_except_name
        )
        generate_residual_plot(
            linear_model = linear_model,
            should_plot_residuals_versus_predicted_values = TRUE
        )
        acf(linear_model$residuals)
        qqnorm(linear_model$residuals)
        qqline(linear_model$residuals, col = "red")
        linear_model <- lm(
            formula = mpg ~ weight,
            data = data_frame_of_columns_except_name
        )
        generate_residual_plot(
            linear_model = linear_model,
            should_plot_residuals_versus_predicted_values = TRUE
        )
        perform_Box_Cox_Method(
            linear_model,
            vector_of_values_of_lambda = seq(-1, 1, 0.1),
            whether_to_plot = TRUE
        )
        linear_model <- lm(
            formula = I(mpg^{-1/2}) ~ weight,
            data = data_frame_of_columns_except_name
        )
        generate_residual_plot(
            linear_model = linear_model,
            should_plot_residuals_versus_predicted_values = TRUE
        )
        acf(linear_model$residuals)
        qqnorm(linear_model$residuals)
        qqline(linear_model$residuals, col = "red")
        linear_model <- lm(
            formula = mpg ~ acceleration,
            data = data_frame_of_columns_except_name
        )
        generate_residual_plot(
            linear_model = linear_model,
            should_plot_residuals_versus_predicted_values = TRUE
        )
        perform_Box_Cox_Method(
            linear_model,
            vector_of_values_of_lambda = seq(-1, 1, 0.1),
            whether_to_plot = TRUE
        )
        linear_model <- lm(
            formula = log(mpg) ~ acceleration,
            data = data_frame_of_columns_except_name
        )
        generate_residual_plot(
            linear_model = linear_model,
            should_plot_residuals_versus_predicted_values = TRUE
        )
        acf(linear_model$residuals)
        qqnorm(linear_model$residuals)
        qqline(linear_model$residuals, col = "red")
        linear_model <- lm(
            formula = mpg ~ year,
            data = data_frame_of_columns_except_name
        )
        generate_residual_plot(
            linear_model = linear_model,
            should_plot_residuals_versus_predicted_values = TRUE
        )
        perform_Box_Cox_Method(
            linear_model,
            vector_of_values_of_lambda = seq(-1, 1, 0.1),
            whether_to_plot = TRUE
        )
        linear_model <- lm(
            formula = sqrt(mpg) ~ year,
            data = data_frame_of_columns_except_name
        )
        generate_residual_plot(
            linear_model = linear_model,
            should_plot_residuals_versus_predicted_values = TRUE
        )
        acf(linear_model$residuals)
        qqnorm(linear_model$residuals)
        qqline(linear_model$residuals, col = "red")
        linear_model <- lm(
            formula = mpg ~ origin,
            data = data_frame_of_columns_except_name
        )
        generate_residual_plot(
            linear_model = linear_model,
            should_plot_residuals_versus_predicted_values = TRUE
        )
        acf(linear_model$residuals)
        qqnorm(linear_model$residuals)
        qqline(linear_model$residuals, col = "red")
        transformed_data_frame <- data_frame_of_columns_except_name
        transformed_data_frame$cylinders <- (transformed_data_frame$cylinders)^4
        correlation_matrix <- cor(transformed_data_frame)
        analyze_correlation_matrix(correlation_matrix)
        linear_model <- lm(
            formula = I(mpg^{-0.5}) ~ I(cylinders^4),
            data = data_frame_of_columns_except_name
        )
        calculate_AIC(linear_model)
        linear_model <- lm(
            formula = I(mpg^{-0.5}) ~ displacement,
            data = data_frame_of_columns_except_name
        )
        calculate_AIC(linear_model)
        simple_linear_regression_model <- lm(
            formula = I(mpg^{-0.5}) ~ I(cylinders^4),
            data = data_frame_of_columns_except_name
        )
        multiple_linear_regression_model <- lm(
            formula = I(mpg^{-0.5}) ~ I(cylinders^4) + I(year^{-1}),
            data = data_frame_of_columns_except_name
        )
        step(
            object = simple_linear_regression_model,
            scope = list(
                lower = simple_linear_regression_model,
                upper = multiple_linear_regression_model
            ),
            direction = "forward"
        )
        library(car)
        vif(multiple_linear_regression_model)
        ```
        
        ```{r}
        multiple_linear_regression_model <- lm(
            formula = I(mpg^{-0.5}) ~ I(cylinders^4) + I(year^{-1}),
            data = data_frame_of_columns_except_name
        )
        plot(multiple_linear_regression_model)
        acf(multiple_linear_regression_model$residuals, main = "ACF Values vs. Lag")
        summarize_linear_model(multiple_linear_regression_model)
        ```

14. This problem focuses on the collinearity problem.

    (a) Perform the following commands in R. 

        ```{r}
        set.seed(1)
        x1 = runif(100)
        x2 = 0.5 * x1 + rnorm(100) / 10
        y = 2 + 2 * x1 + 0.3 * x2 + rnorm(100)
        ```

        The last line corresponds to creating a linear model in which $y$ is a function of $x1$ and $x2$. Write out the form of the linear model. What are the regression coefficients?
        
        $$\boldsymbol{y}\left(\boldsymbol{x}_1, \boldsymbol{x}_2\right) = \boldsymbol{\beta}_0 + \beta_1 \boldsymbol{x}_1 + \beta_2 \boldsymbol{x}_2$$
        $$\boldsymbol{\beta}_0 = 2 + rnorm(100)$$
        $$\beta_1 = 2$$
        $$\beta_2 = 0.3$$
        $$y\left(x_1, x_2\right) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon$$
        $$\beta_0 = 2 + rnorm(1)$$
        $$\beta_1 = 2$$
        $$\beta_2 = 0.3$$

    (b) What is the correlation between `x1` and `x2`? Create a scatterplot displaying the relationship between the variables.

        The correlation between $x_1$ and $x_2$ is $0.835$.
        
        ```{r}
        data_frame <- data.frame(x1 = x1, x2 = x2, y = y)
        cor(data_frame)
        pairs(data_frame)
        ```

    (c) Using this data, fit a least squares regression to predict `y` using `x1` and `x2`. Describe the results obtained. What are $\hat\beta_0$, $\hat\beta_1$, and $\hat\beta_2$? How do these relate to the true $\beta_0$, $\beta_1$, and $\beta_2$? Can you reject the null hypothesis $H_0: \beta_1 = 0$? How about the null hypothesis $H_0: \beta_2 = 0$?
    
        $$\hat{\beta}_0 = 2.131$$
        $$\hat{\beta}_1 = 1.440$$
        $$\hat{\beta}_2 = 1.010$$
        
        These estimates of regression coefficients are the centers of the below $95$ percent confidence intervals for the true regression coefficients.
        
        A critical value $t_{\alpha/2 = 0.05/2, \ df_{Res} = 97} = 1.985$. The summary for the above linear model provides test statistics for predictors. In parallel, the summary provides probabilities where each probability $p$ is the probability that the magnitude $|t|$ of a random test statistic is greater than the magnitude $|t_0|$ of the appropriate test statistic. Because the magnitude of the test statistic for $x_1$ is greater than the critical value, and the probability for $x_1$ is less than the significance level $\alpha = 0.05$, we reject the null hypothesis $H_0: \beta_1 = 0$ and the null hypothesis that $x_1$ is insignificant in predicting the response in the context of the model and can be removed from the model. We have sufficient evidence to support the alternate hypothesis that $x_1$ is significant in predicting the response in the context of the model and cannot be removed from the model. Because the magnitude of the test statistic for $x_2$ is less than the critical value, and the probability for $x_2$ is greater than the significance level $\alpha = 0.05$, we fail to reject the null hypothesis $H_0: \beta_2 = 0$ and the null hypothesis that $x_2$ is insignificant in predicting the response in the context of the model and can be removed from the model. We have insufficient evidence to support the alternate hypothesis that $x_2$ is significant in predicting the response in the context of the model and cannot be removed from the model. We recommend removing $x_2$ from the model.
        
        ```{r}
        linear_model <- lm(formula = y ~ x1 + x2, data = data_frame)
        summarize_linear_model(linear_model)
        confint(object = linear_model, level = 0.95)
        ```

    (d) Now fit a least squares regression to predict `y` using only `x1`. Comment on your results. Can you reject the null hypothesis $H_0: \beta_1 = 0$?
    
        A critical value $t_{\alpha/2 = 0.05/2, \ df_{Res} = 98} = 1.984$. The summary for the above linear model provides test statistics for predictor $x_1$. In parallel, the summary provides probabilities where each probability $p$ is the probability that the magnitude $|t|$ of a random test statistic is greater than the magnitude $|t_0|$ of the appropriate test statistic. Because the magnitude of the test statistic for $x_1$ is greater than the critical value, and the probability for $x_1$ is less than the significance level $\alpha = 0.05$, we reject the null hypothesis $H_0: \beta_1 = 0$ and the null hypothesis that $x_1$ is insignificant in predicting the response in the context of the model and can be removed from the model. We have sufficient evidence to support the alternate hypothesis that $x_1$ is significant in predicting the response in the context of the model and cannot be removed from the model.
    
        ```{r}
        linear_model <- lm(formula = y ~ x1, data = data_frame)
        summarize_linear_model(linear_model)
        ```

    (e) Now fit a least squares regression to predict `y` using only `x2`. Comment on your results. Can you reject the null hypothesis $H_0: \beta_2 = 0$?
    
        A critical value $t_{\alpha/2 = 0.05/2, \ df_{Res} = 98} = 1.984$. The summary for the above linear model provides test statistics for predictor $x_1$. In parallel, the summary provides probabilities where each probability $p$ is the probability that the magnitude $|t|$ of a random test statistic is greater than the magnitude $|t_0|$ of the appropriate test statistic. Because the magnitude of the test statistic for $x_2$ is greater than the critical value, and the probability for $x_2$ is less than the significance level $\alpha = 0.05$, we reject the null hypothesis $H_0: \beta_2 = 0$ and the null hypothesis that $x_2$ is insignificant in predicting the response in the context of the model and can be removed from the model. We have sufficient evidence to support the alternate hypothesis that $x_2$ is significant in predicting the response in the context of the model and cannot be removed from the model.
    
        ```{r}
        linear_model <- lm(formula = y ~ x2, data = data_frame)
        summarize_linear_model(linear_model)
        ```

    (f) Do the results obtained in (c)â€“(e) contradict each other? Explain your answer.
    
        These results do not contradict each other. The significance of predictors in predicting a response is in context to the model as a whole. $x_2$ is significant to a simple linear regression model involving $y$ and $x_2$. $x_2$ is insignificant to a simple linear regression model involving $y$, $x_1$, and $x_2$.

    (g) Now suppose we obtain one additional observation, which was unfortunately mismeasured.
    
        ```{r}
        x1 <- c(x1, 0.1) 
        x2 <- c(x2, 0.8)
        y <- c(y, 6)
        ```
        
        Re-fit the linear models from (c) to (e) using this new data. What effect does this new observation have on the each of the models? In each model, is this observation an outlier? A high-leverage point? Both? Explain your answers.
        
        This new observation for $y\left(x_1, x_2\right) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon$ causes $\beta_0$ to change by a factor of $(\beta_{0, f} - \beta_{0, i})/\beta_{0, i} = (2.2267 - 2.1305)/2.1305 = 0.045$, $\beta_1$ to change by a factor of $-0.625$, and $\beta_2$ to change by a factor of $1.490$. The residual standard error changes by a factor of $0.018$ and by $1$ degree of freedom. The multiple and adjusted coefficients of determination $R^2$ change by factors of $0.048$ and $0.054$. The $F$ statistic changes by a factor of $0.071$ and $0$ and $1$ degrees of freedom. The number of observations changes by $1$. The estimated variance of errors changes by a factor of $0.035$. The prediction coefficient of determination $R^2$ changes by a coefficient of $0.134$.
        
        We use the Student's $t$ distribution and the Bonferroni procedure to find a cutoff value for outlier detection using externally studentized residuals. Let $n = 101$ be the number of observations and let $df_{Res}$ be the number of degrees of freedom for our recommended model. If magnitude of externally studentized residual $|t_i|$ is greater than a critical value $t_{\alpha/(2n), df_{Res} - 1}$, observation $i$ is deemd an outlier. Our new observation is an outlier.
        
        A leverage $h_{ii}$ is used to identify how far observation $i$ is from the centroid of the predictor space. If leverage $h_{ii} > \frac{2p}{n}$, then observation $i$ is deemed to have high leverage and is outlying in the predictor space. High leverage observations are data points that are most likely to be influential. Our observation is a high-leverage observation.
        
        ```{r}
        data_frame <- data.frame(x1 = x1, x2 = x2, y = y)
        linear_model <- lm(formula = y ~ x1 + x2, data = data_frame)
        summarize_linear_model(linear_model)
        significance_level <- 0.05
        number_of_observations <- get_number_of_observations(linear_model)
        residual_degrees_of_freedom <-
            calculate_residual_degrees_of_freedom(linear_model)
        critical_value_tc <- calculate_critical_value_tc(
            significance_level,
            number_of_observations,
            residual_degrees_of_freedom - 1,
            hypothesis_test_is_two_tailed = FALSE
        )
        externally_studentized_residuals <- rstudent(linear_model)
        vector_of_outliers <- externally_studentized_residuals[
            abs(externally_studentized_residuals) > critical_value_tc
        ]
        vector_of_outliers
        leverages <- lm.influence(linear_model)$hat
        number_of_variables <- get_number_of_variables(linear_model)
        high_leverages <-
            leverages[leverages > 2 * number_of_variables / number_of_observations]
        indices_of_high_leverage_observations <- as.numeric(names(high_leverages))
        indices_of_high_leverage_observations
        ```
        
        This new observation for $y\left(x_1\right) = \beta_0 + \beta_1 x_1 + \epsilon$ causes $\beta_0$ to change by a factor of $0.068$ and $\beta_1$ to change by a factor of $-0.106$. The residual standard error changes by a factor of $0.053$ and by $1$ degree of freedom. The multiple and adjusted coefficients of determination $R^2$ change by factors of $-0.228$ and $-0.239$. The $F$ statistic changes by a factor of $-0.263$ and $0$ and $1$ degrees of freedom. The number of observations changes by $1$. The estimated variance of errors changes by a factor of $0.110$. The prediction coefficient of determination $R^2$ changes by a coefficient of $0.300$.
        
        We use the Student's $t$ distribution and the Bonferroni procedure to find a cutoff value for outlier detection using externally studentized residuals. Let $n = 101$ be the number of observations and let $df_{Res}$ be the number of degrees of freedom for our recommended model. If magnitude of externally studentized residual $|t_i|$ is greater than a critical value $t_{\alpha/(2n), df_{Res} - 1}$, observation $i$ is deemd an outlier. Our new observation is an outlier.
        
        A leverage $h_{ii}$ is used to identify how far observation $i$ is from the centroid of the predictor space. If leverage $h_{ii} > \frac{2p}{n}$, then observation $i$ is deemed to have high leverage and is outlying in the predictor space. High leverage observations are data points that are most likely to be influential. Our observation is not a high-leverage observation.
        
        ```{r}
        linear_model <- lm(formula = y ~ x1, data = data_frame)
        summarize_linear_model(linear_model)
        significance_level <- 0.05
        number_of_observations <- get_number_of_observations(linear_model)
        residual_degrees_of_freedom <-
            calculate_residual_degrees_of_freedom(linear_model)
        critical_value_tc <- calculate_critical_value_tc(
            significance_level,
            number_of_observations,
            residual_degrees_of_freedom - 1,
            hypothesis_test_is_two_tailed = FALSE
        )
        externally_studentized_residuals <- rstudent(linear_model)
        vector_of_outliers <- externally_studentized_residuals[
            abs(externally_studentized_residuals) > critical_value_tc
        ]
        vector_of_outliers
        leverages <- lm.influence(linear_model)$hat
        number_of_variables <- get_number_of_variables(linear_model)
        high_leverages <-
            leverages[leverages > 2 * number_of_variables / number_of_observations]
        indices_of_high_leverage_observations <- as.numeric(names(high_leverages))
        indices_of_high_leverage_observations
        ```
        
        This new observation for $y\left(x_1\right) = \beta_0 + \beta_2 x_2 + \epsilon$ causes $\beta_0$ to change by a factor of $-0.019$ and $\beta_2$ to change by a factor of $0.076$. The residual standard error changes by a factor of $0.002$ and by $1$ degree of freedom. The multiple and adjusted coefficients of determination $R^2$ change by factors of $0.204$ and $0.216$. The $F$ statistic changes by a factor of $0.271$ and $0$ and $1$ degrees of freedom. The number of observations changes by $1$. The estimated variance of errors changes by a factor of $0.003$. The prediction coefficient of determination $R^2$ changes by a coefficient of $0.266$.
        
        We use the Student's $t$ distribution and the Bonferroni procedure to find a cutoff value for outlier detection using externally studentized residuals. Let $n = 101$ be the number of observations and let $df_{Res}$ be the number of degrees of freedom for our recommended model. If magnitude of externally studentized residual $|t_i|$ is greater than a critical value $t_{\alpha/(2n), df_{Res} - 1}$, observation $i$ is deemd an outlier. Our new observation is not an outlier.
        
        A leverage $h_{ii}$ is used to identify how far observation $i$ is from the centroid of the predictor space. If leverage $h_{ii} > \frac{2p}{n}$, then observation $i$ is deemed to have high leverage and is outlying in the predictor space. High leverage observations are data points that are most likely to be influential. Our observation is a high-leverage observation.
        
        ```{r}
        linear_model <- lm(formula = y ~ x2, data = data_frame)
        summarize_linear_model(linear_model)
        significance_level <- 0.05
        number_of_observations <- get_number_of_observations(linear_model)
        residual_degrees_of_freedom <-
            calculate_residual_degrees_of_freedom(linear_model)
        critical_value_tc <- calculate_critical_value_tc(
            significance_level,
            number_of_observations,
            residual_degrees_of_freedom - 1,
            hypothesis_test_is_two_tailed = FALSE
        )
        externally_studentized_residuals <- rstudent(linear_model)
        vector_of_outliers <- externally_studentized_residuals[
            abs(externally_studentized_residuals) > critical_value_tc
        ]
        vector_of_outliers
        leverages <- lm.influence(linear_model)$hat
        number_of_variables <- get_number_of_variables(linear_model)
        high_leverages <-
            leverages[leverages > 2 * number_of_variables / number_of_observations]
        indices_of_high_leverage_observations <- as.numeric(names(high_leverages))
        indices_of_high_leverage_observations
        ```

15. This problem involves the Boston data set, which we saw in the lab for this chapter. We will now try to predict per capita crime rate using the other variables in this data set. In other words, per capita crime rate is the response, and the other variables are the predictors.

    (a) For each predictor, fit a simple linear regression model to predict the response. Describe your results. In which of the models is there a statistically significant association between the predictor and the response? Create some plots to back up your assertions.
    
        For a simple linear regression model of $crim$ on a predictor of the Boston data set, the critical value $t_{\alpha/2 = 0.05/2, \ df_{Res} = 504} = 1.965$. The summary for each model provides test statistics for the appropriate predictor. In parallel, the summary provides probabilities where each probability $p$ is the probability that the magnitude $|t|$ of a random test statistic is greater than the magnitude $|t_0|$ of the appropriate test statistic. Because the magnitude of the test statistic for proportion of residential land zoned for lots over $25,000$ square feet $zn$ is greater than the critical value, and the probability for $zn$ is less than the significance level $\alpha = 0.05$, we reject the null hypothesis $H_0: \beta_2 = 0$ and the null hypothesis that $zn$ is insignificant in predicting the response in the context of the model and can be removed from the model. We have sufficient evidence to support the alternate hypothesis that $zn$ is significant in predicting the response in the context of the model and cannot be removed from the model.
    
        Similarly, proportion of non-retail business acres per town $indus$, nitrogen oxides concentration in parts per $10$ million $nox$, average number of rooms per dwelling $rm$, proportion of owner-occupied units built prior to 1940 $age$, weighted mean of distances to five Boston employment centers $dis$, index of accessibility to radial highways $rad$, full-value property-tax rate per $10,000$ dollars $tax$, pupil-teacher ratio by town $ptratio$, percent of population with lower status $lstat$, and median value of owner-occupied homes in thousands of dollars $medv$ are significant. An indicator of whether a census tract bounds the Charles River $chas$ is insignificant.
    
        ```{r}
        colnames(ISLR2::Boston)
        linear_model <- lm(formula = crim ~ chas, data = ISLR2::Boston)
        summarize_linear_model(linear_model)
        plot(x = ISLR2::Boston$nox, y = ISLR2::Boston$crim)
        plot(x = ISLR2::Boston$rm, y = ISLR2::Boston$crim)
        plot(x = ISLR2::Boston$age, y = ISLR2::Boston$crim)
        plot(x = ISLR2::Boston$dis, y = ISLR2::Boston$crim)
        plot(x = ISLR2::Boston$lstat, y = ISLR2::Boston$crim)
        plot(x = ISLR2::Boston$medv, y = ISLR2::Boston$crim)
        ```

    (b) Fit a multiple regression model to predict the response using all of the predictors. Describe your results. For which predictors can we reject the null hypothesis $H_0 : \beta_j = 0$?
    
        For a multiple linear regression model of $crim$ on all predictors of the Boston data set, the critical value $t_{\alpha/2 = 0.05/2, \ df_{Res} = 493} = 1.965$. The summary for this full model provides test statistics for each predictor. In parallel, the summary provides probabilities where each probability $p$ is the probability that the magnitude $|t|$ of a random test statistic is greater than the magnitude $|t_0|$ of the appropriate test statistic. Because the magnitudes of the test statistic for proportion of residential land zoned for lots over $25,000$ square feet $zn$, weighted mean of distances to five Boston employment centers $dis$, index of accessibility to radial highways $rad$, and $medv$ are greater than the critical value, and the probability for $zn$, $dis$, $rad$, and $medv$ are less than the significance level $\alpha = 0.05$, we reject the null hypotheses $H_0: B_j = 0$ for $j \in \{zn, dis, rad, medv\}$ that $zn$, $dis$, $rad$, and median value of owner-occupied homes in thousands of dollars $medv$ are each insignificant in predicting the response in the context of the model and can be removed from the model. We have sufficient evidence to support the alternate hypotheses that $zn$, $dis$, $rad$, and $medv$ are each significant in predicting the response in the context of the model and cannot be removed from the model.
    
        ```{r}
        MLR_model <- lm(
            formula = crim ~ zn + indus + chas + nox + rm + age + dis + rad + tax + ptratio + lstat + medv,
            data = ISLR2::Boston
        )
        summarize_linear_model(MLR_model)
        ```

(c) How do your results from (a) compare to your results from (b)? Create a plot displaying the univariate regression coefficients from (a) on the x-axis, and the multiple regression coefficients from (b) on the y-axis. That is, each predictor is displayed as a single point in the plot. Its coefficient in a simple linear regression model is shown on the x-axis, and its coefficient estimate in the multiple linear regression model is shown on the y-axis.

    Going from simple linear regression models to multiple linear regression models, the factors of change of the coefficients of the predictors $zn$, $indus$, $chas$, $nox$, $rm$, $age$, $dis$, $rad$, $tax$, $ptratio$, $lstat$, and $medv$ are $-1.618$, $-1.114$, $0.564$, $-1.319$, $-1.234$, $-1.008$, $-0.347$, $-0.009$, $-1.127$, $-1.264$, $-0.747$, and $-0.394$. All factors of change are negative. No factors of change are outlying; see below boxplot "Distribution Of Factors Of Change". In "Coefficients Of Predictors Of MLR Model vs. Coefficients Of Predictors Of SLR Model" most points exist a cluster between $-3$ and $3$ and between $-1.5$ and $1.5$. The point representing coefficients for $nox$ is an outlier.

    ```{r}
    index_of_column_crim <- get_index_of_column_of_data_frame(ISLR2::Boston, "crim")
    vector_of_column_names <- colnames(ISLR2::Boston)
    vector_of_names_of_predictors <- vector_of_column_names[-index_of_column_crim]
    vector_of_coefficients_of_predictors_of_SLR_model <- numeric()
    for (column_name in vector_of_names_of_predictors) {
        formula_string <- paste("crim ~ ", column_name, sep = "")
        formula = deserialize_formula_string(formula_string)
        linear_model <- lm(formula = formula, data = ISLR2::Boston)
        vector_of_coefficients_of_SLR_model <- linear_model$coefficients
        index_of_predictor <- get_index_of_label_of_vector(
            vector_of_coefficients_of_SLR_model,
            column_name
        )
        coefficient_of_predictor <-
            vector_of_coefficients_of_SLR_model[[index_of_predictor]]
        vector_of_coefficients_of_predictors_of_SLR_model <- append(
            vector_of_coefficients_of_predictors_of_SLR_model,
            coefficient_of_predictor
        )
    }
    vector_of_coefficients_of_MLR_model <- MLR_model$coefficients
    index_of_label_Intercept <-
        get_index_of_label_of_vector(vector_of_coefficients_of_MLR_model, "(Intercept)")
    vector_of_coefficients_of_predictors_of_MLR_model <-
        vector_of_coefficients_of_MLR_model[-index_of_label_Intercept]
    vector_of_factors_of_change <-
        (
            vector_of_coefficients_of_predictors_of_MLR_model
            - vector_of_coefficients_of_predictors_of_SLR_model
        ) /
        vector_of_coefficients_of_predictors_of_SLR_model
    data_frame <- data.frame(
        vector_of_coefficients_of_predictors_of_SLR_model,
        vector_of_coefficients_of_predictors_of_MLR_model, vector_of_factors_of_change
    )
    print(data_frame)
    plot(
        x = vector_of_coefficients_of_predictors_of_SLR_model,
        y = vector_of_coefficients_of_predictors_of_MLR_model,
        xlab = "coefficient of predictor of SLR model",
        ylab = "coefficient of predictor of MLR model",
        main = "Coefficients Of Predictors Of MLR Model vs.\nCoefficients Of Predictors Of SLR Model"
    )
    plot(
        x = vector_of_coefficients_of_predictors_of_SLR_model,
        y = vector_of_coefficients_of_predictors_of_MLR_model,
        xlim = c(-3, 3),
        ylim = c(-1.5, 1.5),
        xlab = "coefficient of predictor of SLR model",
        ylab = "coefficient of predictor of MLR model",
        main = "Coefficients Of Predictors Of MLR Model vs.\nCoefficients Of Predictors Of SLR Model"
    )
    boxplot(
        vector_of_factors_of_change,
        main = "Distribution Of Factors Of Change",
        ylab = "factor of change"
    )
    ```

    (d) Is there evidence of non-linear association between any of the predictors and the response? To answer this question, for each predictor X, fit a model of the form
        $$Y = \beta_0 +\beta_1X +\beta_2X^2 +\beta_3X^3 + \epsilon$$
        For a multiple linear regression model of $crim$ on powers of a predictor of the Boston data set, the critical value $t_{\alpha/2 = 0.05/2, \ df_{Res} = 504} = 1.965$. The summary for the model for each predictor provides test statistics for the powers of the predictor. In parallel, the summary provides probabilities where each probability $p$ is the probability that the magnitude $|t|$ of a random test statistic is greater than the magnitude $|t_0|$ of the appropriate test statistic. Because the magnitude of the test statistic for the first-degree power of proportion of residential land zoned for lots over $25,000$ square feet $zn$ is greater than the critical value, and the probability for the first-degree power of $zn$ is less than the significance level $\alpha = 0.05$, we reject the null hypothesis $H_0: \beta_1 = 0$ and the null hypothesis that $zn$ is insignificant in predicting the response in the context of the model and can be removed from the model. We have sufficient evidence to support the alternate hypothesis that $zn$ is significant in predicting the response in the context of the model and cannot be removed from the model. Other powers of $zn$ are insignificant. There is no evidence of non-linear association between $zn$ and the response.
    
        First-, second-, and third-degree powers of $indus$ are significant. There is evidence of non-linear association between $indus$ and the response.
        
        First-degree power of $chas$ is significant. Test statistics do not exist for other powers. There is no evidence of non-linear association between $chas$ and the response.
        
        First-, second-, and third-degree powers of $nox$ are significant. There is evidence of non-linear association between $nox$ and the response.
        
        First-, second-, and third-degree powers of $rm$ are insignificant. There is no evidence of non-linear association between $rm$ and the response.
        
        Second- and third-degree powers of $age$ are significant. There is evidence of non-linear association between $age$ and the response.
        
        First-, second-, and third-degree powers of $dis$ are significant. There is evidence of non-linear association between $dis$ and the response.
        
        First-, second-, and third-degree powers of $rad$ are insignificant. There is no evidence of non-linear association between $rad$ and the response.
        
        First-, second-, and third-degree powers of $tax$ are insignificant. There is no evidence of non-linear association between $tax$ and the response.
        
        First-, second-, and third-degree powers of $ptratio$ are significant. There is evidence of non-linear association between $ptratio$ and the response.
        
        First-, second-, and third-degree powers of $lstat$ are insignificant. There is no evidence of non-linear association between $lstat$ and the response.
        
        First-, second-, and third-degree powers of $medv$ are significant. There is evidence of non-linear association between $medv$ and the response.

        ```{r}
        linear_model_of_powers <- lm(
            formula = crim ~ medv + I(medv^2) + I(medv^3),
            data = ISLR2::Boston
        )
        summarize_linear_model(linear_model_of_powers)
        ```